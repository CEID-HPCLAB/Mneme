from Mneme import BlockReader
import pytest
import os
import pickle
import logging
import math


'''
This file contains tests for the BlockReader class from the ParPreprocessor package. 
It includes tests for block offsets loading, storage and computation.
'''

def get_csv_nrows(path: str) -> int:
   from functools import partial
   f = open(path)
   rowcount = sum(chunk.count('\n') for chunk in iter(partial(f.read, 1 << 15), ''))
   return rowcount - 1  # Exclude header

logging.basicConfig(level = logging.INFO)

@pytest.fixture(scope = "module")
def datafile_path() -> str:
   '''
   Pytest fixture to provide the path to the data file.
    
   This fixture constructs the path to a data file located in the "data" directory 
   relative to the directory of the current file. The fixture has module scope, so the path is constructed only once 
   per test module.

   Returns:
      str: The path to the data file.
   '''
    
   # Construct the path to the data file
   datafile_name = "sample_data.csv"
    
   datafile_path = os.path.join(os.path.dirname(__file__), "data", datafile_name)
    
   # Return the path to the data file    
   return datafile_path


@pytest.fixture(scope = "module")
def generate_BlockReader(datafile_path: str) -> BlockReader:
   '''
   A pytest fixture that generates a BlockReader instance.
    
   This fixture creates a BlockReader with a specified number of blocks and a data file path.

   Args:
      datafile_path (str): The path to the data file to be read by the BlockReader.

   Returns:
      BlockReader: The BlockReader instance initialized with the given data file path and number of blocks
   '''
   
   block_offset_save = "./data/block_offsets.dat"
   num_blocks = 120
   block_reader = BlockReader(datafile_path, num_blocks = num_blocks)
   
   logging.debug("\nFixture setup: BlockReader initialized.")    
   
   logging.debug("\nFixture teardown: Deleting binary block offsets file...")
   
   yield block_reader

   try:
      os.remove(block_offset_save)
   except FileNotFoundError:
      logging.error(f"The block offsets file {block_offset_save} was not found. Please ensure the file exists.")
   except PermissionError:
      logging.error(f"You do not have permission to delete the block offsets file {block_offset_save}. Please check your permissions.")
   except Exception as exception:
      logging.error(f"An unexpected error occurred while trying to delete the block offsets file {block_offset_save}: ", exception)

   
def test_offsets_IO(generate_BlockReader: BlockReader, datafile_path: str) -> None:
   '''
   Test to verify if the block offsets are correctly stored and loaded from the corresponding binary file 
   by the BlockReader object.
    
   Args:
      generate_BlockReader (BlockReader): A BlockReader object generated by the fixture.
      datafile_path (str): The path to the data file.
   
   Returns:
      None
   '''
   
   block_offset_save = "./data/block_offsets.dat"
   num_blocks = 120
   
   # Create a BlockReader object with the same number of blocks and data file path as the (generate_BlockReader) fixture
   _ = BlockReader(training_file = datafile_path, num_blocks = num_blocks, block_offset_save = block_offset_save)
   
   # Open the block offsets file and load the offsets
   with open(block_offset_save, "rb") as offsets:
      loaded_offsets = pickle.load(offsets)
   
   # Assert that the block offsets in the created BlockReader object are equal to the loaded offsets
   assert generate_BlockReader.block_offsets == loaded_offsets


@pytest.mark.parametrize("num_blocks", range(1, 200, 20))
def test_offsets_computation(generate_BlockReader: BlockReader, num_blocks: int, datafile_path: str) -> None:
   '''
   Test to verify if the number of offsets computed by the BlockReader matches the expected number of offsets.
    
   Args:
      generate_BlockReader (BlockReader): A BlockReader object generated by the fixture.
      num_blocks (int): The number of blocks to be used for the computation.
      datafile_path (str): The path to the data file.
        
   Returns:
      None
   '''
   
   n_rows = get_csv_nrows(path = datafile_path)
   
   # Calculate the corresponding block size
   block_size = math.ceil(n_rows / num_blocks)
   offsets = [offset for offset in range(1, n_rows, block_size)]
   
   # Create the block offsets using the BlockReader
   created_offsets = generate_BlockReader.create_block_offset_file(path = datafile_path, nrows = n_rows, 
                                                                   block_size = block_size, block_offset_save = '')
   
   # Assert that the number of created offsets is equal to the number of expected offsets
   assert len(offsets) == len(created_offsets)